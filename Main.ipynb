{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\users\\oburg\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\oburg\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from wikipedia) (4.7.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\oburg\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from wikipedia) (2.21.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\oburg\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from beautifulsoup4->wikipedia) (1.7.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\oburg\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oburg\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2018.11.29)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\oburg\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\oburg\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
      "Requirement already satisfied: requests in c:\\users\\oburg\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (2.21.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\oburg\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oburg\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from requests) (2018.11.29)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\oburg\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\oburg\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from requests) (1.24.1)\n",
      "Collecting BeautilfulSoup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Could not find a version that satisfies the requirement BeautilfulSoup (from versions: )\n",
      "No matching distribution found for BeautilfulSoup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Could not find a version that satisfies the requirement time (from versions: )\n",
      "No matching distribution found for time\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\oburg\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (4.3.0)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "#the goal is scrapping wikipedia in order to obtain information about citrus fruit\n",
    "#We take the description section of each one and then we will extract main info.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#A function that gets the URL of the page to be scraped\n",
    "#,gets the html content and uses BeautifulSoup to parse html content\n",
    "\n",
    "def make_soup(link):\n",
    "    get_page = requests.get(link)\n",
    "    html = get_page.content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return  soup\n",
    "\n",
    "\n",
    "#####This function create a list with all the link of the foods in a wikipedia Page.\n",
    "def make_link_list(wiki_page_to_scrap):\n",
    "    start_time = time.time()\n",
    "    link_table = []\n",
    "    soup = make_soup(wiki_page_to_scrap)\n",
    "    table = soup.find('table',{'class':'wikitable'})\n",
    "    \n",
    "            ### This first loop is used to scrap Wiki table Data.\n",
    "    if isinstance(table , bs4.element.Tag):        \n",
    "    \n",
    "        table_cells = []\n",
    "        table = soup.find('table',{'class':'wikitable'})\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cells = row.find_all(['th' , 'td'])\n",
    "            table_cells.append(cells)\n",
    "        \n",
    "        ### This loop is used to locate the \"Common name\" column index in our table cells\n",
    "        indices = []\n",
    "        for j in table_cells:\n",
    "            for i, elem in enumerate(j):\n",
    "                elem = str(elem)\n",
    "                if 'name' in elem:\n",
    "                    indices.append(i)\n",
    "        indice = indices[0]\n",
    "        \n",
    "        #print('Table Cells' , table_cells[:3])\n",
    "        ### Here we implement a loop to keep only the string of the Common name column.\n",
    "        \n",
    "        for cell in table_cells[2:]:\n",
    "            \n",
    "            if (len(cell) < indice) == True :\n",
    "                pass\n",
    "             \n",
    "            else:    \n",
    "                link_table.append(cell[indice].text)\n",
    "            \n",
    "        ### Let's clean the table.\n",
    "        for link in range(len(link_table)):\n",
    "            link_table[link] = link_table[link].strip('\\n')\n",
    "            \n",
    "        print('cpu time for the table schema = {:.4f} sec.'.format(time.time() - start_time))\n",
    "            \n",
    "            ### Here is when the Wiki page is just an Alphabetical List.    \n",
    "    else:\n",
    "        print(soup.find_all('div' , {'class':'div-col'}))\n",
    "        for row in soup.find_all('div' , {'class':'div-col'}):\n",
    "            \n",
    "            for col in row.find_all('li'):\n",
    "                species = col.text\n",
    "                ###We just keep the common name of the species, because only the common name is used in recipes.\n",
    "                only_common_species = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", species) \n",
    "                only_common_species , sep , tail = only_common_species.partition(',')\n",
    "                link_table.append(only_common_species)\n",
    "                \n",
    "                ###Cleaning of the list, we remove all the occurence of string begining by List.\n",
    "        for word in link_table[:]:\n",
    "            if word.find('List') != -1:\n",
    "                link_table.remove(word)\n",
    "            \n",
    "        print('cpu time for the list schema = {:.4f} sec.'.format(time.time() - start_time))     \n",
    "\n",
    "    return link_table  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "cpu time for the list schema = 10.0026 sec.\n",
      "Number of root vegetables 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#page_citrus = 'https://en.wikipedia.org/wiki/List_of_citrus_fruits'\n",
    "#list_citrus = make_link_list(page_citrus)\n",
    "#number_citrus = len(list_citrus)\n",
    "#print('Number of citrus' , len(list_citrus))\n",
    "\n",
    "#page_salads = 'https://en.wikipedia.org/wiki/List_of_leaf_vegetables'\n",
    "#list_salad = make_link_list(page_salads)\n",
    "#number_salad = len(list_salad)\n",
    "#print('Number of salads' , len(list_salad))\n",
    "\n",
    "page_spices = 'https://en.wikipedia.org/wiki/List_of_culinary_herbs_and_spices'\n",
    "#list_spices = make_link_list(page_spices)\n",
    "#number_spices = len(list_spices)\n",
    "#print('Number of spices' , len(list_spices))\n",
    "#print(list_spices[:3])\n",
    "\n",
    "#page_fruit = 'https://simple.wikipedia.org/wiki/List_of_fruits'\n",
    "#list_fruit = make_link_list(page_fruit)\n",
    "#number_fruit = len(list_fruit)\n",
    "#print('Number of fruits', len(list_fruit))\n",
    "\n",
    "#page_herbs = 'https://simple.wikipedia.org/wiki/List_of_herbs'\n",
    "#list_herbs = make_link_list(page_herbs)\n",
    "#number_herbs = len(list_herbs)\n",
    "#print('Number of herbs', len(list_herbs))\n",
    "\n",
    "page_root_vegetable = 'https://en.wikipedia.org/wiki/List_of_root_vegetables'\n",
    "list_root_vegetable = make_link_list(page_root_vegetable)\n",
    "number_root = len(list_root_vegetable)\n",
    "print('Number of root vegetables' , len(list_root_vegetable))\n",
    "print(list_root_vegetable[:4])\n",
    "\n",
    "\n",
    "#print('Total number' , number_citrus + number_fruit + number_herbs + number_root + number_salad + number_spices )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a dictionary with the different categories of vegetables. \n",
    "\n",
    "\n",
    "foods_dict = dict([('CITRUS' , list_citrus) , ('SALAD' , list_salad) , ('SPICES' , list_spices)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu time for the list schema = 0.1300 sec.\n",
      "Number of vegetables 0\n"
     ]
    }
   ],
   "source": [
    "page_vegetable = 'https://simple.wikipedia.org/wiki/List_of_vegetables'\n",
    "list_vegetable = make_link_list(page_vegetable)\n",
    "print('Number of vegetables' , len(list_vegetable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu time for the list schema = 0.1417 sec.\n",
      "List of fruit ['Açaí', 'Apple', 'Akee', 'Apricot'] 114\n"
     ]
    }
   ],
   "source": [
    "page_fruit = 'https://simple.wikipedia.org/wiki/List_of_fruits'\n",
    "\n",
    "list_fruit = make_link_list(page_fruit)\n",
    "print('List of fruit' , list_fruit[:4] , len(list_fruit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
