{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset in the kitchen**\n",
    "\n",
    "\n",
    "### Creation of an ingredient data set by *Olivier Burgaud* (EURECOM 2019), supervised by *Pr. M. Filipone*.\n",
    "\n",
    "I begin my project by studying past projects, and I saw that there was a lack of consistent and widespread dataset to train the algorithm of recipes. \n",
    "\n",
    "Here, I wanted to create a dataset, from one of the widest open source website: ***Wikipedia***. I think it was a good place to find list and set of various and different ingredients. \n",
    "\n",
    "The task was not easy because there is no standard format for ***Wikipedia page***, indeed we can find Table-like page, Alphabetical or ramdomly ordered list. Besides, the content of these tables was not always consistent, for instances we can find hyper text link as \"classical\" text or sentences instead of the ingredient name.\n",
    "I choose to scrap ***Wikipedia page*** with BeautifulSoup library and then I implemented few cleaning function.\n",
    "\n",
    "I create a dictionary with the scrapped data, in order to have a consistent database : type of ingredient are the key, and ingredients are the value. The main advantage of a dictionnary is that is very easy to use, especially with Pandas. Besides, adding a value or a key is straightforward. Last, this format is a built-in schema of Python and it is widely used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First: The scrapping Part\n",
    "\n",
    "I implemented two functions: *make_soup* and *make_link_list*.\n",
    "\n",
    "### *make_soup*\n",
    "\n",
    "This function parses the html page into a BeautifulSoup element. In order to be able to search with html tags.\n",
    "\n",
    "### *make_link_list*\n",
    "\n",
    "Here we extract the information of the BeautifuSoup element. \n",
    "I create 3 differents loop, it depends on the *type* of each wikipedia page, i.e. if it is a Table, an Alphabetical list, or a randomly ordered list. \n",
    "Find and identify between which tags should we extract data, and I didn't achieve to automatize it. I analyzed the source code of pages to know which tag contains the useful data.\n",
    "\n",
    "After that, I creat a list with the data extracted, but, obviously, this data is not clean and I create \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_citrus = 'https://en.wikipedia.org/wiki/List_of_citrus_fruits'\n",
    "page_salads = 'https://en.wikipedia.org/wiki/List_of_leaf_vegetables'\n",
    "page_spices = 'https://en.wikipedia.org/wiki/List_of_culinary_herbs_and_spices'\n",
    "page_fruit = 'https://simple.wikipedia.org/wiki/List_of_fruits'\n",
    "page_herbs = 'https://simple.wikipedia.org/wiki/List_of_herbs'\n",
    "page_vegetable = 'https://simple.wikipedia.org/wiki/List_of_vegetables'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#A function that gets the URL of the page to be scraped\n",
    "#,gets the html content and uses BeautifulSoup to parse html content\n",
    "\n",
    "def make_soup(link):\n",
    "    get_page = requests.get(link)\n",
    "    html = get_page.content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return  soup\n",
    "\n",
    "\n",
    "#####This function create a list with all the link of the foods in a wikipedia Page and it begins the data cleaning.\n",
    "def make_link_list(wiki_page_to_scrap):\n",
    "    start_time = time.time()\n",
    "    link_table = []\n",
    "    soup = make_soup(wiki_page_to_scrap)\n",
    "    table = soup.find('table',{'class':'wikitable'})\n",
    "    \n",
    "            ### This first loop is used to scrap Wiki table Data.\n",
    "    if isinstance(table , bs4.element.Tag):        \n",
    "    \n",
    "        table_cells = []\n",
    "        table = soup.find('table',{'class':'wikitable'})\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cells = row.find_all(['th' , 'td'])\n",
    "            table_cells.append(cells)\n",
    "        \n",
    "        ### This loop is used to locate the \"Common name\" column index in our table cells\n",
    "        indices = []\n",
    "        for j in table_cells:\n",
    "            for i, elem in enumerate(j):\n",
    "                elem = str(elem)\n",
    "                if 'name' in elem:\n",
    "                    indices.append(i)\n",
    "        indice = indices[0]\n",
    "        \n",
    "        ### Here we implement a loop to keep only the string of the Common name column.        \n",
    "        for cell in table_cells[2:]:            \n",
    "            if (len(cell) < indice) == True : ## It is the condition if we have a blank cells i.e there is no common name.\n",
    "                pass\n",
    "             \n",
    "            else:    \n",
    "                link_table.append(cell[indice].text)\n",
    "            \n",
    "        ### We discard all '\\n' tag at the END of the lines.\n",
    "        \n",
    "        for link in range(len(link_table)):\n",
    "            link_table[link] = link_table[link].strip('\\n')\n",
    "            \n",
    "        \n",
    "            \n",
    "        print('cpu time for the table schema = {:.4f} sec.'.format(time.time() - start_time))\n",
    "            \n",
    "            ### Here is when the Wiki page is just an Alphabetical List.    \n",
    "    elif (len(soup.find_all('div' , {'class':'div-col'}))>0) == True : \n",
    "        for row in soup.find_all('div' , {'class':'div-col'}):\n",
    "            \n",
    "            for col in row.find_all('li'):\n",
    "                species = col.text\n",
    "                ###We just keep the common name of the species, because only the common name is used in recipes.\n",
    "                only_common_species = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", species) \n",
    "                only_common_species , sep , tail = only_common_species.partition(',')\n",
    "                link_table.append(only_common_species)\n",
    "                \n",
    "                ###Cleaning of the list, we remove all the occurence of string begining by List.\n",
    "        for word in link_table[:]:\n",
    "            if (word.find('List') != -1) or (word.find('Healthline') != -1) :\n",
    "                link_table.remove(word)\n",
    "            \n",
    "            \n",
    "        print('cpu time for the  Alphabetical list schema = {:.4f} sec.'.format(time.time() - start_time))  \n",
    "        \n",
    "        ### For the list pattern without alphabetical list.\n",
    "    elif (len(soup.find_all('div' , {'class' : 'mw-parser-output'}))> 0 ) == True:\n",
    "        for row in soup.find_all('div' , {'class' : 'mw-parser-output'}):\n",
    "             for col in row.find_all('li'):\n",
    "                    species = col.text\n",
    "        ###We just keep the common name of the species, because only the common name is used in recipes.\n",
    "                    only_common_species = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", species) \n",
    "                    only_common_species , sep , tail = only_common_species.partition(',')\n",
    "                    link_table.append(only_common_species)\n",
    "                    \n",
    "                ###Cleaning of the list, we remove all the occurence of string begining by List.\n",
    "        for word in link_table[:]:\n",
    "            if (word.find('List') != -1) or (word.find('Healthline') != -1):\n",
    "                link_table.remove(word)\n",
    "        \n",
    "        \n",
    "        print('cpu time for the list schema = {:.4f} sec.'.format(time.time() - start_time))            \n",
    "\n",
    "    return link_table  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu time for the table schema = 0.3278 sec.\n",
      "Number of citrus 50\n",
      "cpu time for the table schema = 0.6555 sec.\n",
      "Number of salads 438\n",
      "cpu time for the  Alphabetical list schema = 0.2940 sec.\n",
      "Number of spices 203\n",
      "cpu time for the  Alphabetical list schema = 0.2604 sec.\n",
      "Number of fruits 114\n",
      "cpu time for the  Alphabetical list schema = 0.1630 sec.\n",
      "Number of herbs 49\n",
      "cpu time for the list schema = 0.2084 sec.\n",
      "Number of vegetables 131\n",
      "Total number 985\n"
     ]
    }
   ],
   "source": [
    "list_citrus = make_link_list(page_citrus)\n",
    "number_citrus = len(list_citrus)\n",
    "print('Number of citrus' , len(list_citrus))\n",
    "\n",
    "\n",
    "list_salad = make_link_list(page_salads)\n",
    "number_salad = len(list_salad)\n",
    "print('Number of salads' , len(list_salad))\n",
    "\n",
    "\n",
    "list_spices = make_link_list(page_spices)\n",
    "number_spices = len(list_spices)\n",
    "print('Number of spices' , len(list_spices))\n",
    "\n",
    "\n",
    "list_fruit = make_link_list(page_fruit)\n",
    "number_fruit = len(list_fruit)\n",
    "print('Number of fruits', len(list_fruit))\n",
    "\n",
    "\n",
    "list_herbs = make_link_list(page_herbs)\n",
    "number_herbs = len(list_herbs)\n",
    "print('Number of herbs', len(list_herbs))\n",
    "\n",
    "\n",
    "list_vegetable = make_link_list(page_vegetable)\n",
    "number_vegetable = len(list_vegetable)\n",
    "print('Number of vegetables' , number_vegetable)\n",
    "\n",
    "\n",
    "\n",
    "print('Total number' , number_citrus + number_fruit + number_herbs + number_salad + number_spices + number_vegetable )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Cleaning function of the dictionary\n",
    "### Few common error in the categories:\n",
    "    #html tag as \"\\n\"\n",
    "    \n",
    "def cleaner(list_of_ingre):\n",
    "    spliter_list = []\n",
    "    cleaned_list = []\n",
    "    for i in list_of_ingre:\n",
    "        if '\\n' in i : \n",
    "            spliter_list.append(i.split('\\n'))        \n",
    "        else:\n",
    "            cleaned_list.append(i.capitalize())\n",
    "    if spliter_list != []:\n",
    "        clean_ingre_list = list(np.hstack(spliter_list))\n",
    "    \n",
    "        for ingre in clean_ingre_list:\n",
    "            ingre = ingre.capitalize()\n",
    "            cleaned_list.append(ingre)\n",
    "    cleaned_list = list(set(cleaned_list))\n",
    "    cleaned_list = list(filter(None , cleaned_list))\n",
    "    cleaned_list.sort()\n",
    "    return(cleaned_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a dictionary with the different categories of vegetables. \n",
    "\n",
    "### This function is the constructor of the dictionnary, when we want to add a category and a list (which can be empty)to the dict.\n",
    "def add_cat_to_dict(list_of_ingr , category , food_dict , existency = True):\n",
    "    CAT_NAME = category.upper()\n",
    "    food_dict.update({CAT_NAME : list_of_ingr})\n",
    "    return food_dict\n",
    "    \n",
    "###This function permits to the user to add an element in a category, I thought that the user will add ingredient\n",
    "###one by one, so he should put one tuple (category, ingredient) as input:\n",
    "#def add_instance\n",
    "    \n",
    "def add_ingre_to_dict(ingredient , category , food_dict):\n",
    "    CTG = category.upper()\n",
    "    ingre = ingredient.capitalize()\n",
    "    print(ingre)\n",
    "    #First we check if the category exist.\n",
    "    if CTG in food_dict:        \n",
    "        if ingre in food_dict[CTG]:\n",
    "            print(\"This ingredient is already in the category.\")\n",
    "        else:\n",
    "            food_dict[CTG].append(ingre)\n",
    "            food_dict = food_dict[CTG].sort()\n",
    "    else :\n",
    "        print('This category does not exist, you can create a new one with the function add_cat_to_dict.')\n",
    "    return food_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "+ BeautifulSoup to Download   : https://pypi.org/project/beautifulsoup4/\n",
    "+ BeautifulSoup documentation : https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Olivier\\n']\n"
     ]
    }
   ],
   "source": [
    "f = ['' , 'Olivier\\n']\n",
    "for word in f[:]:\n",
    "    \n",
    "    word = word.strip('\\n')\n",
    "            #if (word.find('List') != -1) or (word.find('Healthline') != -1):\n",
    "             #   f.remove(word)\n",
    "                \n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PastiS', 'Ricard']\n",
      "Alcool\n",
      "['Alcool', 'PastiS', 'Ricard']\n"
     ]
    }
   ],
   "source": [
    "a='PastiS\\nRicard'\n",
    "a= a.split('\\n')\n",
    "print(a)\n",
    "a.append('alcool')\n",
    "c = a[2].capitalize()\n",
    "a[2] = c\n",
    "print(c)\n",
    "a.sort()\n",
    "print(a)\n",
    "#b = a.append('Alcool')\n",
    "#print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oui ['PastiS', 'Ricard']\n",
      "propre\n",
      "propre\n"
     ]
    }
   ],
   "source": [
    "liste_test = ['PastiS\\nRicard' , 'EURECOM' , 'OlivierIng']\n",
    "\n",
    "h = len(liste_test)\n",
    "\n",
    "for i in range(h):\n",
    "    if '\\n' in liste_test[i]:\n",
    "        temp = liste_test[i].split('\\n')\n",
    "        print('oui' , temp)\n",
    "    else:\n",
    "        print('propre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'Olivier': [185, 22]}\n",
      "{'Olivier': [185, 22, 'Thomas']}\n",
      "{'Olivier': [185, 22, 'Thomas'], 'Alcool': 'PAstis'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2c616bf5a5eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdico\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Alcool'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m'PAstis'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdico\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdico\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Alcool'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'biere'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "dico=dict()\n",
    "print(dico)\n",
    "dico.update({'Olivier' : [185 , 22]})\n",
    "print (dico)\n",
    "dico['Olivier'].append('Thomas')\n",
    "print(dico)\n",
    "dico.update({'Alcool' : 'PAstis'})\n",
    "print(dico)\n",
    "dico['Alcool'].append('biere')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
