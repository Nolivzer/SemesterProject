{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset in the kitchen**\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "### Creation of an ingredient data set by *Olivier Burgaud* (EURECOM 2019), supervised by *Pr. M. Filipone*.\n",
    "----\n",
    "\n",
    "I begin my project by studying past projects, and I saw that there was a lack of consistent and widespread dataset to train the algorithm of recipes. \n",
    "\n",
    "Here, I wanted to create a dataset, from one of the widest open source website: ***Wikipedia***. I think it was a good place to find list and set of various and different ingredients. \n",
    "\n",
    "The task was not easy because there is no standard format for ***Wikipedia page***, indeed we can find Table-like page, Alphabetical or ramdomly ordered list. Besides, the content of these tables was not always consistent, for instances we can find hyper text link as \"classical\" text or sentences instead of the ingredient name.\n",
    "I choose to scrap ***Wikipedia page*** with BeautifulSoup library and then I implemented few cleaning function.\n",
    "\n",
    "I create a dictionary with the scrapped data, in order to have a consistent database : type of ingredient are the key, and ingredients are the value. The main advantage of a dictionnary is that is very easy to use, especially with Pandas. Besides, adding a value or a key is straightforward. Last, this format is a built-in schema of Python and it is widely used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The scrapping Part\n",
    "\n",
    "This is the first step of the database creation. The goal of this step is gather data directly from *Wikipedia*. It is the first step of data cleaning and the sketch of the database architecture.\n",
    "After, a fine analysis of several *Wikipedia* page, I acknowledge the fact that there exists 3 mains schema for these pages : *table* , *alphabetical list* and *randomly ordered list* (not totally random in fact, but ordered on different criteria).\n",
    "I implemented two functions: *make_soup* and *make_link_list*.\n",
    "\n",
    "### *make_soup*\n",
    "\n",
    "This function parses the html page into a *BeautifulSoup element*. In order to be able to search with html tags.\n",
    "It returns a *BeautifulSoup element*.\n",
    "\n",
    "### *make_link_list*\n",
    "\n",
    "Here we extract the information of the BeautifuSoup element. \n",
    "I create 3 differents loop, it depends on the *type* of each wikipedia page, i.e. if it is a *table*, an *alphabetical list*, or a *randomly ordered list*. \n",
    "Find and identify between which tags should we extract data, and I didn't achieve to automatize it. I analyzed the source code of several \"typical\" pages to know which tag contains the useful data.\n",
    "\n",
    "After that, I create a list with the data extracted, but, obviously, this data is not clean and I have to clean it.\n",
    "That why I implement a *for* loop to begin the cleaning.\n",
    "\n",
    "It returns a Python *list* and the length of the scrapping, it is *cpu_time*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#A function that gets the URL of the page to be scraped,gets the html content \n",
    "#and uses BeautifulSoup to parse html content.\n",
    "\n",
    "def make_soup(link):\n",
    "    get_page = requests.get(link)\n",
    "    html = get_page.content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return  soup\n",
    "\n",
    "\n",
    "#####This function create a list with all the link of the foods in a wikipedia Page and it begins the data cleaning.\n",
    "def make_link_list(wiki_page_to_scrap):\n",
    "    start_time = time.time()\n",
    "    link_table = []\n",
    "    soup = make_soup(wiki_page_to_scrap)\n",
    "    table = soup.find('table',{'class':'wikitable'})\n",
    "    \n",
    "            ### This first loop is used to scrap Wiki table Data.\n",
    "    if isinstance(table , bs4.element.Tag):        \n",
    "        temp = []\n",
    "        table_cells = []\n",
    "        table = soup.find('table',{'class':'wikitable'})\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cells = row.find_all(['th' , 'td'])\n",
    "            table_cells.append(cells)\n",
    "        \n",
    "        ### This loop is used to locate the \"Common name\" column index in our table cells\n",
    "        indices = []\n",
    "        for j in table_cells:\n",
    "            for i, elem in enumerate(j):\n",
    "                elem = str(elem)\n",
    "                if ('name' in elem) and (not 'Scientific' in elem) :\n",
    "                    indices.append(i)\n",
    "        indice = indices[0]\n",
    "        \n",
    "        ### Here we implement a loop to keep only the string of the Common name column.        \n",
    "        for cell in table_cells[2:]:            \n",
    "            if (len(cell) < indice) == True : ## It is the condition if we have a blank cells i.e there is no common name.\n",
    "                pass\n",
    "             \n",
    "            else:    \n",
    "                temp.append(cell[indice].text)\n",
    "            \n",
    "        ### We discard all '\\n' tag at the END of the lines and we keep only the common name.\n",
    "        for link in range(len(temp)):\n",
    "            temp[link] = temp[link].strip('\\n')            \n",
    "            links = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", temp[link]) \n",
    "            links , sep , tail = links.partition(',')\n",
    "            link_table.append(links)\n",
    "        \n",
    "            \n",
    "        #print('cpu time for the table schema = {:.4f} sec.'.format(time.time() - start_time))\n",
    "        cpu_time = time.time()-start_time\n",
    "            \n",
    "            ### Here is when the Wiki page is just an Alphabetical List.    \n",
    "    elif (len(soup.find_all('div' , {'class':'div-col'}))>0) == True : \n",
    "        for row in soup.find_all('div' , {'class':'div-col'}):\n",
    "            \n",
    "            for col in row.find_all('li'):\n",
    "                species = col.text\n",
    "                ###We just keep the common name of the species, because only the common name is used in recipes.\n",
    "                only_common_species = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", species) \n",
    "                only_common_species , sep , tail = only_common_species.partition(',')\n",
    "                link_table.append(only_common_species)\n",
    "                \n",
    "                ###Cleaning of the list, we remove all the occurence of string begining by List.\n",
    "        for word in link_table[:]:\n",
    "            if (word.find('List') != -1) or (word.find('Healthline') != -1) :\n",
    "                link_table.remove(word)\n",
    "            \n",
    "            \n",
    "        cpu_time = time.time()-start_time\n",
    "        ### For the list pattern without alphabetical list.\n",
    "    elif (len(soup.find_all('div' , {'class' : 'mw-parser-output'}))> 0 ) == True:\n",
    "        for row in soup.find_all('div' , {'class' : 'mw-parser-output'}):\n",
    "             for col in row.find_all('li'):\n",
    "                    species = col.text\n",
    "        ###We just keep the common name of the species, because only the common name is used in recipes.\n",
    "                    only_common_species = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", species) \n",
    "                    only_common_species , sep , tail = only_common_species.partition(',')\n",
    "                    link_table.append(only_common_species)\n",
    "                    \n",
    "                ###Cleaning of the list, we remove all the occurence of string begining by List.\n",
    "        for word in link_table[:]:\n",
    "            if (word.find('List') != -1) or (word.find('Healthline') != -1):\n",
    "                link_table.remove(word)\n",
    "           \n",
    "        \n",
    "        cpu_time = time.time()-start_time\n",
    "    return (link_table , cpu_time)  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning functions\n",
    "\n",
    "### *tagger_cleaner*\n",
    "\n",
    "This function find \"\\n\" tags that stays into string. After the location it separates the string with \"\\n\" as separator. Then, it returns the list with all the ingredients.\n",
    "\n",
    "### *remove_meta*\n",
    "\n",
    "This function discards each occurence of a meta element, i.e. the class or type name, that begins by a number (\"*1 fish, 2 Roe ...*\")because ingredient with a number in its name does not exist. It returns the list that we give as input, without meta elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Cleaning function of the dictionary\n",
    "### Few common error in the categories:\n",
    "    #html tag as \"\\n\"\n",
    "    \n",
    "def tagger_cleaner(list_of_ingre):\n",
    "    start_time = time.time()    \n",
    "    spliter_list = []\n",
    "    cleaned_list = []\n",
    "    for i in list_of_ingre:\n",
    "        if '\\n' in i : \n",
    "            spliter_list.append(i.split('\\n'))        \n",
    "        else:\n",
    "            cleaned_list.append(i.capitalize())\n",
    "    if spliter_list != []:\n",
    "        clean_ingre_list = list(np.hstack(spliter_list))\n",
    "    \n",
    "        for ingre in clean_ingre_list:\n",
    "            ingre = ingre.capitalize()\n",
    "            cleaned_list.append(ingre)\n",
    "    cleaned_list = list(set(cleaned_list))\n",
    "    cleaned_list = list(filter(None , cleaned_list))\n",
    "    cleaned_list.sort()\n",
    "    cpu_time = cpu_time = time.time()-start_time\n",
    "    return(cleaned_list , cpu_time)\n",
    "\n",
    "### This function remove every elements with a numbers in the string.\n",
    "def remove_meta(list_of_ingre): \n",
    "    start_time = time.time()\n",
    "    temp_list = []\n",
    "    temp_ingre_list = copy.deepcopy(list_of_ingre)\n",
    "    for i in (range(len(list_of_ingre))):        \n",
    "        for c in list_of_ingre[i]:\n",
    "            if c.isdigit():\n",
    "                temp_list.append(i)\n",
    "    \n",
    "    temp_list = list(set(temp_list))\n",
    "    for j in temp_list:\n",
    "        temp_ingre_list.remove(list_of_ingre[j])\n",
    "    \n",
    "    cpu_time = time.time()-start_time\n",
    "    return(temp_ingre_list, cpu_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Database creation\n",
    "\n",
    "Here I create two function for the creation of the database, with the construction of a dictionnary. I implemented two user-friendly function, to allow everyone to add a category or just one element to one category.\n",
    "\n",
    "\n",
    "### *add_cat_to_dict*\n",
    "\n",
    "This function permits to add a full category to a dictionary and it verifies if this category does not exist already. The user have to give a *list_of_ingre*, a *category* name and the name of the dictionary as input, the function will return the *food_dict* with the new category if the *category* name does not exist. Else it prints a message to redirect the user to the 2nd function.\n",
    "I create a function to do this, instead of hardcode a dictionnary in order to give the maximum freedom for the user. Indeed he can create dictionnary as he wants, for instance a dictionnary for the fruit, another for the vegetables...\n",
    "\n",
    "### *add_ingre_to_dict*\n",
    "\n",
    "Here user can add just one ingredient in one category. The user just need to put the *ingredient*, the *category* where the *ingredient* should belongs. \n",
    "Besides this function check if the *category*, the *ingredient* are already in the dictionnary. If not, it returns a message to redirect the user to use *add_cat_to_dict*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a dictionary with the different categories of vegetables. \n",
    "\n",
    "### This function is the constructor of the dictionnary, when we want to add a category and a list (which can be empty)to the dict.\n",
    "def add_cat_to_dict(list_of_ingr , category , food_dict):\n",
    "\n",
    "    CAT_NAME = category.upper()\n",
    "    if CAT_NAME in food_dict:\n",
    "        print(\"This category exists already, please use the function add_ingre_to_dict\")\n",
    "    else:\n",
    "        food_dict.update({CAT_NAME : list_of_ingr})\n",
    "        return \n",
    "\n",
    "###This function permits to the user to add an element in a category, I thought that the user will add ingredient\n",
    "###one by one.\n",
    "    \n",
    "def add_ingre_to_dict(ingredient , category , food_dict):\n",
    "    CTG = category.upper()\n",
    "    ingre = ingredient.capitalize()\n",
    "    #First we check if the category exist.\n",
    "    if CTG in food_dict:        \n",
    "        if ingre in food_dict[CTG]:\n",
    "            print(\"This ingredient is already in the category.\")\n",
    "        else:\n",
    "            food_dict[CTG].append(ingre)\n",
    "            food_dict = food_dict[CTG].sort()\n",
    "    else :\n",
    "        print('This category does not exist, you can create a new one with the function add_cat_to_dict.')\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Application\n",
    "\n",
    "Now, let's how we can create a *dictionary* of 8 different types of ingredients.\n",
    "These types are:\n",
    "+ Citrus\n",
    "+ Leaf vegetables (*salads* is the generic term)\n",
    "+ Culinary herbs and spices (*spices* is the generic term)\n",
    "+ Fruit\n",
    "+ Herbs\n",
    "+ Vegetables\n",
    "+ Edible flowers\n",
    "+ Seafood\n",
    "\n",
    "Besides I create a list for each execution time, in order to have an idea of the duration of the processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL identification\n",
    "\n",
    "This step is the foudation of the database creation. The user have to find some *Wikipedia* page to find some data to scrap. My project is far from perfect: the user must find a \"good\" page to scrap : a *Wikipedia List_of_...* page, and not all page works with my functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_citrus = 'https://en.wikipedia.org/wiki/List_of_citrus_fruits'\n",
    "page_salads = 'https://en.wikipedia.org/wiki/List_of_leaf_vegetables'\n",
    "page_spices = 'https://en.wikipedia.org/wiki/List_of_culinary_herbs_and_spices'\n",
    "page_fruit = 'https://simple.wikipedia.org/wiki/List_of_fruits'\n",
    "page_herbs = 'https://simple.wikipedia.org/wiki/List_of_herbs'\n",
    "page_vegetable = 'https://simple.wikipedia.org/wiki/List_of_vegetables'\n",
    "page_edible_flower = 'https://en.wikipedia.org/wiki/List_of_edible_flowers'\n",
    "page_seafood = 'https://en.wikipedia.org/wiki/List_of_types_of_seafood'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List creation\n",
    "\n",
    "Here I create some list of ingredients. I don't scrap meat because the name of each piece can variate very much according to the country, so the data should be not relevant. To fix this problem, we can think about two approaches:\n",
    "+ Analyze a set of recipes with meat and find which name are relevant.\n",
    "+ Create different category in the dictionary depending on the country (huge set will be created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_citrus , t_citrus = make_link_list(page_citrus)\n",
    "\n",
    "list_salad , t_salad = make_link_list(page_salads)\n",
    "\n",
    "list_spices , t_spices = make_link_list(page_spices)\n",
    "\n",
    "list_fruit , t_fruit = make_link_list(page_fruit)\n",
    "\n",
    "list_herbs , t_herbs = make_link_list(page_herbs)\n",
    "\n",
    "list_vegetable , t_vegetable = make_link_list(page_vegetable)\n",
    "\n",
    "list_edible_flower , t_edible_flower = make_link_list(page_edible_flower)\n",
    "\n",
    "list_seafood , t_seafood = make_link_list(page_seafood)\n",
    "\n",
    "time_list = [t_citrus , t_salad , t_spices , t_fruit , t_herbs , t_vegetable , t_edible_flower , t_seafood]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning step\n",
    "\n",
    "Now, each list created will be cleaned with the cleaning functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I applied the 2 cleaning functions for each list.\n",
    "list_citrus , t_citrus = tagger_cleaner(list_citrus)\n",
    "time_list[0] += t_citrus # Updating the time of processing\n",
    "list_citrus , t_citrus = remove_meta(list_citrus)\n",
    "time_list[0] += t_citrus\n",
    "\n",
    "list_salad , t_salad = tagger_cleaner(list_salad)\n",
    "time_list[1] += t_salad\n",
    "list_salad , t_salad = remove_meta(list_salad)\n",
    "time_list[1] += t_salad\n",
    "\n",
    "list_spices , t_spices = tagger_cleaner(list_spices)\n",
    "time_list[2] += t_spices\n",
    "list_spices , t_spices = remove_meta(list_spices)\n",
    "time_list[2] += t_spices\n",
    "\n",
    "list_fruit , t_fruit = tagger_cleaner(list_fruit)\n",
    "time_list[3] += t_fruit\n",
    "list_fruit , t_fruit = remove_meta(list_fruit)\n",
    "time_list[3] += t_fruit\n",
    "\n",
    "list_herbs , t_herbs = tagger_cleaner(list_herbs)\n",
    "time_list[4] += t_herbs\n",
    "list_herbs , t_herbs = remove_meta(list_herbs)\n",
    "time_list[4] += t_herbs\n",
    "\n",
    "list_vegetable , t_vegetable = tagger_cleaner(list_vegetable)\n",
    "time_list[5] += t_vegetable\n",
    "list_vegetable , t_vegetable = remove_meta(list_vegetable)\n",
    "time_list[5] += t_vegetable\n",
    "\n",
    "list_edible_flower , t_edible_flower = tagger_cleaner(list_edible_flower)\n",
    "time_list[6] += t_edible_flower\n",
    "list_edible_flower , t_edible_flower = remove_meta(list_edible_flower)\n",
    "time_list[6] += t_edible_flower\n",
    "\n",
    "list_seafood , t_seafood = tagger_cleaner(list_seafood)\n",
    "time_list[7] += t_seafood\n",
    "list_seafood , t_seafood = remove_meta(list_seafood)\n",
    "time_list[7] += t_seafood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary creation\n",
    "\n",
    "Now we will add each category and its instances, to a dictionnary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['CITRUS', 'SALADS', 'SPICES', 'FRUITS', 'HERBS', 'VEGETABLES', 'EDIBLE_FLOWERS', 'SEAFOOD'])\n",
      "50\n",
      "The whole process took 2.3745 sec.\n"
     ]
    }
   ],
   "source": [
    "food_dict = dict()\n",
    "\n",
    "add_cat_to_dict(list_citrus , 'citrus' , food_dict)\n",
    "add_cat_to_dict(list_salad , 'Salads', food_dict)\n",
    "add_cat_to_dict(list_spices , 'spices' , food_dict)\n",
    "add_cat_to_dict(list_fruit , 'FRUITs' , food_dict)\n",
    "add_cat_to_dict(list_herbs , 'herbs' , food_dict)\n",
    "add_cat_to_dict(list_vegetable , 'vegetables' , food_dict)\n",
    "add_cat_to_dict(list_edible_flower , 'edible_flowers' , food_dict)\n",
    "add_cat_to_dict(list_seafood , 'seafood' , food_dict)\n",
    "print(food_dict.keys())\n",
    "print(len(food_dict['EDIBLE_FLOWERS']))\n",
    "print('The whole process took {:.4f} sec.'.format(sum(time_list)))\n",
    "#print('The dictionary contains {:}'.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionnary contains 1091\n"
     ]
    }
   ],
   "source": [
    "nb_citrus = len(food_dict['CITRUS']) \n",
    "nb_salads = len(food_dict['SALADS'])\n",
    "nb_spices = len(food_dict['SPICES'])\n",
    "nb_fruits = len(food_dict['FRUITS'])\n",
    "nb_herbs = len(food_dict['HERBS'])\n",
    "nb_vegetables = len(food_dict['VEGETABLES'])\n",
    "nb_flowers  = len(food_dict['EDIBLE_FLOWERS'])\n",
    "nb_seafood = len(food_dict['SEAFOOD']) \n",
    "\n",
    "print('The dictionnary contains {:}'.format(nb_citrus + nb_salads + nb_spices + nb_fruits + nb_herbs + nb_vegetables + nb_flowers + nb_seafood ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project teach me how to scrap website, and how to clean this type of data. , First I was thinking it should be an easy task, but it is a very difficult task, the *Wikipedia* pages are very different and schema can be totally different too. \n",
    "\n",
    "Difficulties:\n",
    "\n",
    "+ Find how to extract data from *Wikipedia* pages\n",
    "+ Create function that fit well for each schema, without falling into overfitting\n",
    "+ Clean the data, indeed what is the most straightforward method to define an ingredient and how to define an ingredient?\n",
    "\n",
    "I do not deal with the spelling of ingredient, indeed the user can add name with a mistake, and my function will add it to the dictionnary as a new one (for instance, *opple* instead of *apple*). But I see in the project of *Huang* that he tackles this problem.\n",
    "\n",
    "With hindsight, I think I would rather use a real object-oriented way to do the scrapping. I did not have enough time to do it. But my code should be improvable in this sense. So, if I continue my project I will begin by create class and method, then I should optimize the code, because I feel that it is not efficient. Another step, could be the automatization of several steps in the process (automatize the *List creation*, *Cleaning step* and *Database creation*). I think the *Cleaning step* should be perfectible, with *NLTK* Python library, I hint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "+ BeautifulSoup to Download   : https://pypi.org/project/beautifulsoup4/\n",
    "+ BeautifulSoup documentation : https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "+ https://medium.com/@Alexander_H/scraping-wikipedia-with-python-8000fc9c9e6c\n",
    "+ https://roche.io/2016/05/scrape-wikipedia-with-python\n",
    "+ https://www.shiveshskitchen.com/2015/09/classification-of-vegetables.html \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
